{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cheik\\anaconda3\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "C:\\Users\\Cheik\\anaconda3\\lib\\site-packages\\nltk\\lm\\counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "C:\\Users\\Cheik\\anaconda3\\lib\\site-packages\\nltk\\lm\\vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Counter, Iterable\n",
      "C:\\Users\\Cheik\\anaconda3\\lib\\site-packages\\botocore\\vendored\\requests\\packages\\urllib3\\_collections.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n",
      "C:\\Users\\Cheik\\anaconda3\\lib\\site-packages\\botocore\\vendored\\requests\\packages\\urllib3\\_collections.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n",
      "C:\\Users\\Cheik\\anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#mon_env_gdn\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib as plt \n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import spacy\n",
    "import tweepy as tw\n",
    "import textblob_fr\n",
    "import textblob\n",
    "#os.chdir(r\"C:\\Users\\csamassa\\Desktop\\Mémoire\\Nouveau GDN\")\n",
    "os.chdir(r\"C:\\Users\\Cheik\\Desktop\\mémoire\")\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 34.682637214660645 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "demo=pd.read_csv(\"DEMOCRATIE_ET_CITOYENNETE.csv\", \n",
    "                 sep=\",\",\n",
    "                 #nrows=10,\n",
    "                 usecols=[0,10,2,11,13,14,16,17,19,20,22,23,25,26,27,29,30,31,32,33,34,35,36,37,38,39,40,42,43,44,45,46,47]\n",
    "                 ,dtype={\"authorZipCode\":object}\n",
    "                 )\n",
    "fisc=pd.read_csv(\"LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv\", \n",
    "                 sep=\",\",\n",
    "                 #nrows=10,\n",
    "                 usecols=[0,10,2,11,12,13,14,15,16,17,18]\n",
    "                 ,dtype={\"authorZipCode\":object}\n",
    "                 )\n",
    "eco=pd.read_csv(\"LA_TRANSITION_ECOLOGIQUE.csv\", \n",
    "                sep=\",\",\n",
    "                #nrows=10,               \n",
    "                usecols=[0,10,2,11,12,14,16,17,18,20,22,23,24,25,26]\n",
    "                ,dtype={\"authorZipCode\":object}\n",
    "                )\n",
    "                \n",
    "org=pd.read_csv(\"ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv\", \n",
    "                sep=\",\",\n",
    "               #nrows=10,\n",
    "                usecols=[0,10,2,11,13,15,16,19,20,21,24,25,27,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]\n",
    "                ,dtype={\"authorZipCode\":object}\n",
    "               )\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.382740020751953 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# On rajoute une colonne comportant le thème de chaque question\n",
    "demo.insert(column=\"Thème\",value=\"DEMOCRATIE ET CITOYENNETE\",loc=3)\n",
    "fisc.insert(column=\"Thème\",value=\"LA FISCALITE ET LES DEPENSES PUBLIQUES\",loc=3)\n",
    "eco.insert(column=\"Thème\",value=\"LA TRANSITION ECOLOGIQUE\",loc=3)\n",
    "org.insert(column=\"Thème\",value=\"ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES\",loc=3)\n",
    "\n",
    "\n",
    "# On nettoie le début des questions\n",
    "def clean_question(df):\n",
    "    colonnes=df.columns\n",
    "    colonnes1=[re.sub(pattern=r\"\\bQ[A-Za-z0-9]+\\s+\\-\\s\",repl='',string=nom) for nom in colonnes]\n",
    "    return(colonnes1)\n",
    "\n",
    "\n",
    "\n",
    "# On applique tt en même temps\n",
    "demo.columns,fisc.columns,eco.columns,org.columns=clean_question(demo),clean_question(fisc),clean_question(eco),clean_question(org)\n",
    "col1=[\"id\", \"authorZipCode\",\"Thème\"]\n",
    "\n",
    "def empiller(df):\n",
    "    stack_0=df.loc[:, ~df.columns.isin(col1)].stack(dropna=False) # prend les QO (toutes les questions sauf celles de col1)\n",
    "    stack_1=stack_0.reset_index()                                 # On supp l'index pour avoir le level 0 pour la future jointure\n",
    "    stack_2=stack_1.merge(df[col1],left_on=\"level_0\",right_index=True,how=\"left\")\n",
    "    stack_2.columns=[\"idx_0\",\"Question\",\"Réponse\",\"id\",\"authorZipCode\",\"Thème\"] #idx_0 c'est le numéro de la ligne dans le fichier original de chaque thème \n",
    "    stack_2.dropna(inplace=True)\n",
    "    return stack_2\n",
    "\n",
    "\n",
    "demo_1,fisc_1,eco_1,org_1=empiller(demo),empiller(fisc),empiller(eco),empiller(org)\n",
    "\n",
    "del(demo,fisc,eco,org)# On supp les variables inutiles de l'environnement\n",
    "\n",
    "contributions=pd.concat([demo_1,fisc_1,eco_1,org_1],axis=0)   #On met tout dans un même df\n",
    " \n",
    "contributions.reset_index(drop=True,inplace=True)\n",
    "\n",
    "del(demo_1,fisc_1,eco_1,org_1,col1)\n",
    "\n",
    "contributions.drop(columns=\"id\", inplace=True) # On supp la colonne (on a qu'à utiliser idx_0 si on veut la trace des contributions)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx_0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Réponse</th>\n",
       "      <th>authorZipCode</th>\n",
       "      <th>Thème</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>Les augmentations de rémunérations</td>\n",
       "      <td>79190</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>En qui faites-vous le plus confiance pour vous...</td>\n",
       "      <td>Le citoyen</td>\n",
       "      <td>79190</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Y a-t-il d'autres points sur la démocratie et ...</td>\n",
       "      <td>Afin d’éviter de creuser les inégalités ne plu...</td>\n",
       "      <td>79190</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>title</td>\n",
       "      <td>rénover l'enquête publique pour en faire un vr...</td>\n",
       "      <td>01800</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>En qui faites-vous le plus confiance pour vous...</td>\n",
       "      <td>Un instrument de démocratie locale à modernise...</td>\n",
       "      <td>01800</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963582</th>\n",
       "      <td>111952</td>\n",
       "      <td>Quand vous pensez à l'évolution des services p...</td>\n",
       "      <td>Je n'en vois pas beaucoup, mais je constate pa...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963583</th>\n",
       "      <td>111952</td>\n",
       "      <td>Quels sont les services publics qui doivent le...</td>\n",
       "      <td>La SNCF , la renationalisation et en refaire u...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963584</th>\n",
       "      <td>111952</td>\n",
       "      <td>Si oui, comment ?</td>\n",
       "      <td>En les laissant faire preuve de bon sens avec ...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963585</th>\n",
       "      <td>111952</td>\n",
       "      <td>Si oui, comment ?</td>\n",
       "      <td>En simplifiant les normes tout en maintenant u...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963586</th>\n",
       "      <td>111952</td>\n",
       "      <td>Si vous avez été amené à demander un rembourse...</td>\n",
       "      <td>Des relevés de remboursement papier trop rares</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5963587 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx_0                                           Question  \\\n",
       "0             0                                              title   \n",
       "1             0  En qui faites-vous le plus confiance pour vous...   \n",
       "2             0  Y a-t-il d'autres points sur la démocratie et ...   \n",
       "3             1                                              title   \n",
       "4             1  En qui faites-vous le plus confiance pour vous...   \n",
       "...         ...                                                ...   \n",
       "5963582  111952  Quand vous pensez à l'évolution des services p...   \n",
       "5963583  111952  Quels sont les services publics qui doivent le...   \n",
       "5963584  111952                                  Si oui, comment ?   \n",
       "5963585  111952                                  Si oui, comment ?   \n",
       "5963586  111952  Si vous avez été amené à demander un rembourse...   \n",
       "\n",
       "                                                   Réponse authorZipCode  \\\n",
       "0                       Les augmentations de rémunérations         79190   \n",
       "1                                               Le citoyen         79190   \n",
       "2        Afin d’éviter de creuser les inégalités ne plu...         79190   \n",
       "3        rénover l'enquête publique pour en faire un vr...         01800   \n",
       "4        Un instrument de démocratie locale à modernise...         01800   \n",
       "...                                                    ...           ...   \n",
       "5963582  Je n'en vois pas beaucoup, mais je constate pa...         50260   \n",
       "5963583  La SNCF , la renationalisation et en refaire u...         50260   \n",
       "5963584  En les laissant faire preuve de bon sens avec ...         50260   \n",
       "5963585  En simplifiant les normes tout en maintenant u...         50260   \n",
       "5963586     Des relevés de remboursement papier trop rares         50260   \n",
       "\n",
       "                                                    Thème  \n",
       "0                               DEMOCRATIE ET CITOYENNETE  \n",
       "1                               DEMOCRATIE ET CITOYENNETE  \n",
       "2                               DEMOCRATIE ET CITOYENNETE  \n",
       "3                               DEMOCRATIE ET CITOYENNETE  \n",
       "4                               DEMOCRATIE ET CITOYENNETE  \n",
       "...                                                   ...  \n",
       "5963582  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963583  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963584  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963585  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963586  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "\n",
       "[5963587 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.670011819288  caractères en moyennes par contribution\n",
      "10.0  % des contributions contiennent 19.0 caractères ou moins\n",
      "20.0  % des contributions contiennent 35.0 caractères ou moins\n",
      "30.000000000000004  % des contributions contiennent 52.0 caractères ou moins\n",
      "40.0  % des contributions contiennent 72.0 caractères ou moins\n",
      "50.0  % des contributions contiennent 97.0 caractères ou moins\n",
      "60.0  % des contributions contiennent 129.0 caractères ou moins\n",
      "70.0  % des contributions contiennent 173.0 caractères ou moins\n",
      "80.0  % des contributions contiennent 239.0 caractères ou moins\n",
      "90.0  % des contributions contiennent 388.0 caractères ou moins\n"
     ]
    }
   ],
   "source": [
    "# Nombre de caractères dans chaque contribution en moyenne\n",
    "contributions[\"taille_str\"]=contributions.loc[contributions[\"Question\"] !=\"title\"][\"Réponse\"].str.len()#On exclu les questions titres\n",
    "print(contributions[\"taille_str\"].mean(),\" caractères en moyennes par contribution\")\n",
    "\n",
    "# La moyenne est sensible aux extrêmes donc on va plutôt afficher des déciles pour voir comment ça se réparti\n",
    "\n",
    "deciles=pd.DataFrame(contributions[\"taille_str\"].quantile(q=np.arange(0.1,1.0,0.1))) # des quantiles allant de 0.1 à 0.9\n",
    "deciles.reset_index(inplace=True)\n",
    "deciles.columns=[\"déciles\",\"Nombre de caractères\"]\n",
    "\n",
    "for index,row in deciles.iterrows():\n",
    "    print(row[\"déciles\"]*100,\" % des contributions contiennent\", row[\"Nombre de caractères\"], \"caractères ou moins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27b521e8448>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#deciles.reset_index(inplace=True)\n",
    "#deciles.columns=[\"déciles\",\"nb_caract\"]\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(x=deciles[\"Nombre de caractères\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "\n",
    "#contributions=pd.read_csv('contributions.csv',dtype={\"authorZipCode\":object})\n",
    "\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def compte_group(df,nom,fichier=None,export=False):\n",
    "    resultat=df.loc[df[\"Réponse\"].str.contains(nom,case=False, regex=True)]\n",
    "    ###On va mettre chaque thème dans une feuille différente\n",
    "    ##D'abord on filtre les résultats par thèmes\n",
    "    resultat[\"nb_occurr\"]=resultat.loc[:,\"Réponse\"].str.count(pat=nom, flags=re.I)\n",
    "    if export==True:\n",
    "        resultat_1=resultat.loc[resultat[\"Thème\"]==\"DEMOCRATIE ET CITOYENNETE\"]\n",
    "        resultat_2=resultat.loc[resultat[\"Thème\"]==\"LA FISCALITE ET LES DEPENSES PUBLIQUES\"]\n",
    "        resultat_3=resultat.loc[resultat[\"Thème\"]==\"LA TRANSITION ECOLOGIQUE\"]\n",
    "        resultat_4=resultat.loc[resultat[\"Thème\"]==\"ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES\"] # je sais que c'est public mais changer ça est trop chiant à faire pzrce que faudra changer le nom des thèmes (ou y'a la faute aussi)\n",
    "    \n",
    "        writer = pd.ExcelWriter(fichier+'.xlsx', engine='xlsxwriter')\n",
    "    \n",
    "        resultat_1.to_excel(writer, sheet_name=\"DEMOCRATIE\")\n",
    "        resultat_2.to_excel(writer, sheet_name=\"FISCALITE\")\n",
    "        resultat_3.to_excel(writer, sheet_name=\"TRANSITION_ECOLOGIQUE\")\n",
    "        resultat_4.to_excel(writer, sheet_name=\"ORGANISATION_DE_LETAT\") \n",
    "    \n",
    "        writer.save()\n",
    "    return(resultat)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va prendre un exemple pour essayer notre code: toutes les contributions du GDN qui contiennent les mots pandémie, épidémie sras, coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "corona=compte_group(contributions,r\"\\bpand[e-é]mi[a-z]\\b|\\b[e-é]pid[e-é]mi[a-z]\\b|\\bsras\\b|\\bcoronavirus\\b\")\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#DF series.apply est plus rapide que df.apply\n",
    "\n",
    "#On ne  prend que les contributions de plus d'une phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "aleo=corona[\"Réponse\"].apply(nltk.sent_tokenize)\n",
    "essai=pd.DataFrame(aleo.apply(pd.Series).stack()).reset_index()#la fonction series fait que le séparateur devient la virgule entre chaque éléments de la liste, le stack fait qu'on les empile, on a un multi index où chaque index de level 0= numéro de la contribution, level 1= numéro de la phrase dans la contribution\n",
    "\n",
    "#On ajoute le nombre de phrases de chaque contributions\n",
    "essai_gpby=essai.groupby(by=\"level_0\",as_index=False)[0].count()\n",
    "\n",
    "essai=essai.merge(essai_gpby,on=\"level_0\")\n",
    "\n",
    "\n",
    "essai.columns=[\"level_0\",\"nb_sent\",\"sent\",\"nb_sent_total\"]\n",
    "contain=essai.loc[essai[\"sent\"].str.contains(r\"\\bpand[e-é]mi[a-z]\\b|\\b[e-é]pid[e-é]mie\\b|\\bsras\\b|\\bcoronavirus\\b\",regex=True,case=False)]\n",
    "\n",
    "# On met dans un dictionnaire l'index de chaque phrase contenant notre pattern\n",
    "dico_lvl_contain=dict(zip(contain.index,zip(contain[\"nb_sent\"],contain[\"nb_sent_total\"])))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#DF series.apply est plus rapide que df.apply\n",
    "\n",
    "#On ne  prend que les contributions de plus d'une phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque phrase qui contient les mots qu'on veut, on prend celle d'avant et celle d'après. \n",
    "Si jamais la phrase qui contient les patterns est la première, python va renvoyer une erreur lorsqu'on va lui demander \n",
    "de prendre la phrase d'avant vu qu'il n'y a rien avant, viceversa si c'est après. \n",
    "Pour résoudre cela, on va dire à python d'exclure la première lorsqu'on lui demandera de prendre les phrases x-1 et la dernière phrase quand on lui demande de prendre les phrases x+1 (encore une fois si y'a rien après x on va avoir une erreur index out of bounds). \n",
    "On a selectionné dans une autre opérationles phrases qui contiennent le pattern, cette opération a pour but de prendre x-1 et x+1 pas x. \n",
    "Si la première phrase contient le pattern, python ne va prendre que celle qui la suit, et si dernière phrase contient le pattern, python ne va prendre que celle qui la précède, \n",
    "Si la première et la deuxième phrase contiennent le patern, python va prendre celle qui la suit (x+1), et la ligne suivante python va de nouveau prendre la première (x-1, qui contient le pattern),  et la troisème (x+1), la deuxième étant x sur cette ligne (x+1 la ligne précédente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "idx_avant=[[k-1,k,v] for (k,v) in dico_lvl_contain.items() if v[0]>0] #prend la phrase d'avant si la phrase  (celle qui matche le pattern) n'est pas la première de la contribution, donc d'index supérieur à 0\n",
    "phrase_avant=essai.loc[[x[0] for x in idx_avant],]\n",
    "phrase_avant[\"id_x\"]=essai.loc[[x[1] for x in idx_avant],].index # On ajoute l'index des phrases qui contiennent la phrase du patern(celle d'après du coup si répond aux conditions du for)\n",
    "phrase_avant.columns=[\"level_0\",\"nb_sent_av\", \"sent_av\",\"nb_sent_total\",\"id_x\"]\n",
    "\n",
    "idx_apres=[[k+1,k,v] for (k,v) in dico_lvl_contain.items() if v[0]<(v[1]-1)] # # prend la phrase d'après si la phrase n'est pas la dernière de la contribution, donc d'index inférieure au nombre de phrases dans la contribution. on ajoute moins 1 parce que v[1] c'est la taille par ex 20 et v[0] c'est l'index (0:19) donc faut mettre v[1]-1 pour que ça corresponde\n",
    "\n",
    "phrase_apres=essai.loc[[x[0] for x in idx_apres],]\n",
    "phrase_apres[\"id_x\"]=essai.loc[[x[1] for x in idx_apres],].index #On ajoute l'index des phrases du pattern (celle d'avant du coup si matche les conditions)\n",
    "phrase_apres.columns=[\"level_0\",\"nb_sent_ap\", \"sent_ap\",\"nb_sent_total\",\"id_x\"]\n",
    "\n",
    "contain=contain.rename_axis('id_x').reset_index() #On ajoute la colonne de l'id pour la jointure\n",
    "essai_final=contain.merge(phrase_avant, on=\"id_x\", how=\"outer\").merge(phrase_apres,on=\"id_x\",how=\"outer\") #On fait une union des trois dataframes\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai_final.loc[:,[\"sent_av\",\"sent\",\"sent_ap\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai_final['sent_av']=essai_final['sent_av'].astype(str).replace('nan','')\n",
    "essai_final['sent_ap']=essai_final['sent_ap'].astype(str).replace('nan','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai_final[\"phrase_av_ap\"]=essai_final[['sent_av', 'sent','sent_ap']].apply(lambda x: ''.join(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai_final.loc[:,\"phrase_av_ap\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import fr_core_news_sm\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "nlp = fr_core_news_sm.load()\n",
    "stop_list=[\"oui\",\"non\",\"faut\",\"faudrait\",\"une\",\"qu\",\"il\",\"faut\",\"y\",\"faire\",\"faudrait\",\" \"]\n",
    "\n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True\n",
    "    \n",
    "\"\"\"   \n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\"\"\"\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "#nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp1=fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in tqdm(essai_final[\"phrase_av_ap\"]):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)    \n",
    "    doc_list.append(pr)\n",
    "\n",
    "print(doc_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnaire = gensim.corpora.Dictionary(doc_list) # dictionnaire des tokens uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnaire.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionnaire.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_130 = corpus[130]\n",
    "for i in range(len(doc_130)):\n",
    "    print(\"Le mot {} (\\\"{}\\\") aparait {} fois.\".format(doc_130[i][0], \n",
    "                                               dictionnaire[doc_130[i][0]], \n",
    "doc_130[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionnaire,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation modèle\n",
    "\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_list, dictionary=dictionnaire, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionnaire)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Twitter####\n",
    "# initialize api instance\n",
    "auth = tw.OAuthHandler(consumer_key='',\n",
    "                        consumer_secret=')\n",
    "\n",
    "auth.set_access_token('',\n",
    "                        '')\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords=[\"pandemie\",\"pandémie\",\"epidémie\",\"sras\",\"coronavirus\",\"épidémies\",\"epidemies\",\"pandémie\",\"pandémies\",\"covid\",\"COVID\", \"Covid\"]\n",
    "OU=\" OR \"\n",
    "search_words= OU.join([\"#\"+x for x in swords])\n",
    "search_words+=\" AND (@gouvernementFR OR @Elysee OR @EmmanuelMacron OR @Matignon OR @JeanCASTEX OR @MinSoliSante OR @olivierveran OR @AssembleeNat)\"\n",
    "search_words+=\" -filter:links\"\n",
    "search_words+=\" AND -filter:retweets\"\n",
    "search_words+=\" AND -filter:replies\"\n",
    "search_date=\"2020-08-23\"\n",
    "print(search_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On prend le premier élément de la liste (on va considérer que si le tweet devait être labelisé par un hashtag, ce serait le premier mentionné), et on extrait les tags des dictionnaires avec leur clé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"fr\",\n",
    "              since=\"2020-08-22\", tweet_mode='extended').items(100)\n",
    "def tweet_recup(tweets):\n",
    "    tweet_list=[]\n",
    "    for tweet in tweets:\n",
    "        tweet_text=tweet.full_text\n",
    "        tweed_id_str=tweet.id_str\n",
    "        location=tweet.user.location\n",
    "        username = tweet.user.screen_name\n",
    "        acctdesc = tweet.user.description\n",
    "        following = tweet.user.friends_count\n",
    "        followers = tweet.user.followers_count\n",
    "        totaltweets = tweet.user.statuses_count\n",
    "        tweetcreatedts = tweet.created_at\n",
    "        retweetcount = tweet.retweet_count\n",
    "        hashtags = tweet.entities['hashtags']\n",
    "        usercreatedts = tweet.user.created_at\n",
    "        dico={\"tweet_text\":tweet_text,\n",
    "                         \"tweed_id_str\":tweed_id_str,\n",
    "                         \"tweetcreatedts\":tweetcreatedts,\n",
    "                         \"location\":location,\n",
    "                         \"username\":username,\n",
    "                         \"acctdesc\":acctdesc,\n",
    "                         \"following\":following,\n",
    "                         \"followers\":followers,\n",
    "                         \"totaltweets\":totaltweets,\n",
    "                         \"usercreatedts\":usercreatedts,\n",
    "                         \"retweetcount\": retweetcount,\n",
    "                         \"hashtags\": hashtags}\n",
    "        tweet_list.append(dico)\n",
    "    tweets_df=pd.DataFrame(tweet_list,columns=list(dico.keys()))\n",
    "\n",
    "    return tweets_df\n",
    "\n",
    "df=tweet_recup(tweets)\n",
    "#df.to_excel(r\"C:\\Users\\Cheik\\OneDrive - OCCURRENCE\\Memoire-master\\tweets_30_08.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hashtags\"].apply(lambda x: x[0][\"text\"]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class PreProcessTweets:\n",
    "    def clean_tweet(self):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return ' '.join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", self).split())\n",
    "    \n",
    "    def stopwords(self):\n",
    "        self._stopwords = nlp(self)\n",
    "    \"\"\"     \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "    \"\"\"\n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub(r'@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "        return [word for word in tweet if word not in self._stopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les caractères spéciaux de nos tweets pour permettre au modèle d'analyse de sentiment qu'on va utiliser de reconnaitre le texte. Spécificité française: on va garder les voyelles avec des trémas, des circonflexes et des accents. Il ne s'agit pas de caractères spéciaux en français. \n",
    "\n",
    "On  fait également le choix de ne pas supprimer les hashtags mentionnés dans les tweets. Il semble qu'ils aient un sens et une valeur significatifs pour l'analyse de sentiments. \n",
    "(le 23/07): Certains tweets ont des tonalités contradictoires, peut-être est-ce les hashtags qui faussent les résultats\n",
    "\n",
    "On va donc essayer de les supprimer pour voir s'il y a du mieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "def test (df): ### le 23/07\n",
    "    \n",
    "    def clean_tweet(string):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        pre=' '.join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z(àâäéèêëùûüïîôö) \\t])|(\\w+:\\/\\/\\S+)\", \" \", string).split())\n",
    "        \n",
    "        return pre.lower()\n",
    "        \n",
    "    def sentiment_analyzer (text):\n",
    "        c=TextBlob(text,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer())\n",
    "        return c.sentiment[0]\n",
    "   \n",
    "    def df_clean_sentiment(df1):\n",
    "        df[\"clean_text\"]=df[\"tweet_text\"].apply(lambda x: clean_tweet(x))\n",
    "        df[\"sentiment\"]=df[\"clean_text\"].apply(lambda x: sentiment_analyzer(x))\n",
    "        \n",
    "        df.loc[df[\"sentiment\"]>0,\"label\"]=\"positif\" # On met cette information dans une autre colonne et pas en remplaçant la valeur numérique pour pouvoir nuancer la positivité ou négativité après\n",
    "        df.loc[df[\"sentiment\"]==0,\"label\"]=\"neutre\"\n",
    "        df.loc[df[\"sentiment\"]<0,\"label\"]=\"négatif\"\n",
    "        df[\"hashtags\"]=df[\"hashtags\"].apply(lambda x: x[0][\"text\"]) # On prend le premier élément de la liste (qui est le seul élement finalement), et on extrait les tags des dictionnaires avec leur cl\n",
    "\n",
    "        #df[\"label_sentiment\"]=df[\"label_sentiment\"]\n",
    "        return(df)\n",
    "    \n",
    "    return(df_clean_sentiment(df)) # On appelle notre fonction\n",
    "#test(df).to_excel(r\"C:\\Users\\Cheik\\OneDrive - OCCURRENCE\\Memoire-master\\30_08_sentiments.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.figure(figsize=(8,10))\n",
    "ax=sns.countplot(y=\"hashtags\",data=df).set_title(\"Nombre de tweets par hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pourcentage de tweets par sentiments\n",
    "pcent_pos=(len(df.loc[df[\"label\"]==\"positif\"])/len(df))*100\n",
    "\n",
    "pcent_neg=(len(df.loc[df[\"label\"]==\"négatif\"])/len(df))*100\n",
    "\n",
    "pcent_neut=(len(df.loc[df[\"label\"]==\"neutre\"])/len(df))*100\n",
    "\n",
    "dico_sentiment={\"positif\": (len(df.loc[df[\"label\"]==\"positif\"])/len(df))*100,\n",
    "                \"negatif\": (len(df.loc[df[\"label\"]==\"négatif\"])/len(df))*100,\n",
    "                \"neutre\":  (len(df.loc[df[\"label\"]==\"neutre\"])/len(df))*100}\n",
    "\n",
    "for k,v in dico_sentiment.items():\n",
    "    print('Le sentiment associé aux tweets est à {} % {}'.format(v,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hashtags par polarité moyenne\n",
    "sent_hashtag=df.groupby(by=\"hashtags\",as_index=False).mean().loc[:,[\"hashtags\",\"sentiment\"]]\n",
    "\n",
    "plt.pyplot.figure(figsize=(8,10))\n",
    "sns.barplot(y=sent_hashtag[\"hashtags\"],x=sent_hashtag[\"sentiment\"], orient=\"h\").set_title(\"Hashtags par polarité moyenne\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords et punct removal perso, mais on va plutôt utiliser celui de Spacy\n",
    "'''\n",
    "def stop_words(liste=False):\n",
    "    stopwords=pd.read_csv(\"stopwords.csv\",sep=';')\n",
    "    nvx_stpwds=pd.DataFrame(liste,dtype=str)\n",
    "    stopwords=pd.concat([stopwords[\"a\"],nvx_stpwds[0]],axis=0,ignore_index=True)\n",
    "    stopwords=pd.concat([stopwords,stopwords.str.upper(),stopwords.str.capitalize()],axis=0, ignore_index=True)\n",
    "    \n",
    "    return(stopwords.tolist())\n",
    "stopwords=stop_words(['oui','non','faut','faudrait','une','qu','il'])\n",
    "\n",
    "\n",
    "essai_final[\"text_processed\"] = essai_final[\"phrase_av_ap\"].map(lambda x: re.sub('[,\\.!?:]', ' ', x))\n",
    "essai_final[\"text_processed\"]=essai_final[\"text_processed\"].map(lambda x:' '.join([word for word in x.split() if word not in (stopwords)]))#On enlève les stopwords\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "    \n",
    "###LDA_multicore\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=5, id2word=dictionnaire, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    \n",
    "#évaluation modèle\n",
    "liste_topic=[]\n",
    "for index, score in sorted(lda_model[corpus[50]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 1)))\n",
    "    liste_topic.append((score, lda_model.print_topic(index, 1))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Pour les grosses requêtes\n",
    "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'tweed_id_str','acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'tweet_text', 'hashtags']\n",
    "                                )\n",
    "    program_start = time.time()\n",
    "    \n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tw.Cursor(api.search, q=search_words, lang=\"fr\", since=search_date, tweet_mode='extended').items(numTweets)\n",
    "        ##tweets = tw.Cursor(api.search, q=search_words, lang=\"fr\", since=search_date, tweet_mode='extended').items(numTweets)\n",
    "# Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "# Obtain the following info (methods to call them out):\n",
    "        # user.screen_name - twitter handle\n",
    "        # user.description - description of account\n",
    "        # user.location - where is he tweeting from\n",
    "        # user.friends_count - no. of other users that user is following (following)\n",
    "        # user.followers_count - no. of other users who are following this user (followers)\n",
    "        # user.statuses_count - total tweets by user\n",
    "        # user.created_at - when the user account was created\n",
    "        # created_at - when the tweet was created\n",
    "        # retweet_count - no. of retweets\n",
    "        # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "        # retweeted_status.full_text - full text of the tweet\n",
    "        # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "# Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "        for tweet in tweet_list:\n",
    "# Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            tweed_id_str=tweet.id_str\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "            try:\n",
    "                tweet_text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                tweet_text = tweet.full_text\n",
    "# Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [username, tweed_id_str, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, tweet_text, hashtags]\n",
    "# Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "# increase counter - noTweets  \n",
    "            noTweets += numTweets\n",
    "        # Run ended:\n",
    "            end_run = time.time()\n",
    "            duration_run = round((end_run-start_run)/60, 2)\n",
    "        \n",
    "            print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "            print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "            time.sleep(900) #15 minute sleep time\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "  from datetime import datetime\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    export_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "# Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path+\"tweets\"+export_timestamp+\".xlsx\"\n",
    "# Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.excel(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))\n",
    "\"\"\"\n",
    "    db_tweets[\"hashtags\"]=db_tweets[\"hashtags\"].apply(lambda x: x[0][\"text\"]) # On prend le premier élément de la liste (qui est le seul élement finalement), et on extrait les tags des dictionnaires avec leur cl\n",
    "    return(db_tweets)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tweets=scraptweets(search_words=search_words, date_since=\"2020-08-22\", numTweets=15, numRuns=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"###Grosse fonction pour occurrence à retravailler\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "class ProcessTweets:\n",
    "    def clean_tweet(self):\n",
    "        '''\n",
    "        Fonction de nettoyage de tweets, supprimant les liens, les caractères spéciaux en utilisant des regex et supprimant les stopwords, \n",
    "        la ponctuation, et tokenizant  en utilisant nlp() de spacy (auquel on ajouté la suppression des stopwords)\n",
    "        '''\n",
    "        pre=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z(àâäéèêëùûüïîôö) \\t])|(\\w+:\\/\\/\\S+)\", \" \", self).split())\n",
    "        return pre.lower\n",
    "    def df_clean(df):\n",
    "        text=pd.DataFrame(df[\"tweet_text\"].apply(lambda x: PreProcessTweets.clean_tweet(x)))\n",
    "        return(text)\n",
    "    \n",
    "    def get_tweet_sentiment(tweet):\n",
    "        '''\n",
    "        Utility function to classify sentiment of passed tweet\n",
    "        using textblob's sentiment method\n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text\n",
    "       # analysis=text.apply(lambda tweet: TextBlob(tweet).sentiment)\n",
    "        polarité=TextBlob(tweet,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer())\n",
    "            \n",
    "        if polarité.sentiment.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif polarité.sentiment.polarity == 0:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constat assez mitigé pour notre modèle d'analyse de sentiments. Les tweets désignéss par Textblob comme étant négatifs, semblent corrects (autrement dit je les aurais aussi catégorisés comme négatifs). Pour les tweets positifs en revanche, je ne suis pas tout à fait convaincu. A vrai dire j'ai beaucoup de mal à les faire rentrer dans une catégorie par conséquent j'en aurais mis une bonne partie dans les neutres. Toutefois, il n'est pas scandaleux de voir qu'ils ont une polarité supérieure à 0 (peuvent être interprétés comme ayant un sentiment positif) d'autant que la polarité des tweets positifs les plus \"discutables\" est très proche de 0.\n",
    "Je pense donc que plutôt que regarder les labels (positfs, négatifs ou neutres) il fauat regarder la polarité. Du coup ce que je vais sûrement faire c'est ajouter des catégories intermédiaires par exemple (peu positif, très positif assez positif) etc... Pour nuancer les sentiment et analyser plus précisément. \n",
    "\n",
    "Vérifier aussi si dans mon expression régulière les caractères que je décide de garder ne biaisent pas peu le sentiment idem pour les hastags que j'ai décidé de garder\n",
    "En outre, il serait intéressant de voir si je fais les mêmes constats sur un corpus de tweets plus important en utilisant la grosse fonction plus haut. A voir....\n",
    "Désormais je vais faire quelques statistiques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
