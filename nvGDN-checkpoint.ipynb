{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "#os.chdir(r\"C:\\Users\\csamassa\\Desktop\\Mémoire\\Nouveau GDN\")\n",
    "os.chdir(r\"C:\\Users\\Cheik\\Desktop\\mémoire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo=pd.read_csv(\"DEMOCRATIE_ET_CITOYENNETE.csv\", \n",
    "                 sep=\",\",\n",
    "                 #nrows=10,\n",
    "                 usecols=[0,10,2,11,13,14,16,17,19,20,22,23,25,26,27,29,30,31,32,33,34,35,36,37,38,39,40,42,43,44,45,46,47]\n",
    "                 ,dtype={\"authorZipCode\":object}\n",
    "                 )\n",
    "fisc=pd.read_csv(\"LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv\", \n",
    "                 sep=\",\",\n",
    "                 #nrows=10,\n",
    "                 usecols=[0,10,2,11,12,13,14,15,16,17,18]\n",
    "                 ,dtype={\"authorZipCode\":object}\n",
    "                 )\n",
    "eco=pd.read_csv(\"LA_TRANSITION_ECOLOGIQUE.csv\", \n",
    "                sep=\",\",\n",
    "                #nrows=10,               \n",
    "                usecols=[0,10,2,11,12,14,16,17,18,20,22,23,24,25,26]\n",
    "                ,dtype={\"authorZipCode\":object}\n",
    "                )\n",
    "                \n",
    "org=pd.read_csv(\"ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv\", \n",
    "                sep=\",\",\n",
    "               #nrows=10,\n",
    "                usecols=[0,10,2,11,13,15,16,19,20,21,24,25,27,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]\n",
    "               ,dtype={\"authorZipCode\":object}\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On rajoute une colonne comportant le thème de chaque question\n",
    "demo.insert(column=\"Thème\",value=\"DEMOCRATIE ET CITOYENNETE\",loc=3)\n",
    "fisc.insert(column=\"Thème\",value=\"LA FISCALITE ET LES DEPENSES PUBLIQUES\",loc=3)\n",
    "eco.insert(column=\"Thème\",value=\"LA TRANSITION ECOLOGIQUE\",loc=3)\n",
    "org.insert(column=\"Thème\",value=\"ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES\",loc=3)\n",
    "\n",
    "\n",
    "# On nettoie le début des questions\n",
    "def clean_question(df):\n",
    "    colonnes=df.columns\n",
    "    colonnes1=[re.sub(pattern=r\"\\bQ[A-Za-z0-9]+\\s+\\-\\s\",repl='',string=nom) for nom in colonnes]\n",
    "    return(colonnes1)\n",
    "\n",
    "\n",
    "\n",
    "# On applique tt en même temps\n",
    "demo.columns,fisc.columns,eco.columns,org.columns=clean_question(demo),clean_question(fisc),clean_question(eco),clean_question(org)\n",
    "col1=[\"id\", \"authorZipCode\",\"Thème\"]\n",
    "\n",
    "def empiller(df):\n",
    "    stack_0=df.loc[:, ~df.columns.isin(col1)].stack(dropna=False) # prend les QO (toutes les questions sauf celles de col1)\n",
    "    stack_1=stack_0.reset_index()                                 # On supp l'index pour avoir le level 0 pour la future jointure\n",
    "    stack_2=stack_1.merge(df[col1],left_on=\"level_0\",right_index=True,how=\"left\")\n",
    "    stack_2.columns=[\"idx_0\",\"Question\",\"Réponse\",\"id\",\"authorZipCode\",\"Thème\"] #idx_0 c'est le numéro de la ligne dans le fichier original de chaque thème \n",
    "    stack_2.dropna(inplace=True)\n",
    "    return stack_2\n",
    "\n",
    "\n",
    "demo_1,fisc_1,eco_1,org_1=empiller(demo),empiller(fisc),empiller(eco),empiller(org)\n",
    "\n",
    "del(demo,fisc,eco,org)# On supp les variables inutiles de l'environnement\n",
    "\n",
    "contributions_emp=pd.concat([demo_1,fisc_1,eco_1,org_1],axis=0)   #On met tout dans un même df\n",
    " \n",
    "contributions_emp.reset_index(drop=True,inplace=True)\n",
    "\n",
    "del(demo_1,fisc_1,eco_1,org_1,col1)\n",
    "\n",
    "contributions_emp.drop(columns=\"id\", inplace=True) # On supp la colonne (on a qu'à utiliser idx_0 si on veut la trace des contributions)\n",
    "\n",
    "contributions_emp.to_csv('contributions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compte_group(df,nom,fichier=None,export=False):\n",
    "    resultat=df.loc[df[\"Réponse\"].str.contains(nom,case=False, regex=True)]\n",
    "    ###On va mettre chaque thème dans une feuille différente\n",
    "    ##D'abord on filtre les résultats par thèmes\n",
    "   \n",
    "    if export==True:\n",
    "        resultat_1=resultat.loc[resultat[\"Thème\"]==\"DEMOCRATIE ET CITOYENNETE\"]\n",
    "        resultat_2=resultat.loc[resultat[\"Thème\"]==\"LA FISCALITE ET LES DEPENSES PUBLIQUES\"]\n",
    "        resultat_3=resultat.loc[resultat[\"Thème\"]==\"LA TRANSITION ECOLOGIQUE\"]\n",
    "        resultat_4=resultat.loc[resultat[\"Thème\"]==\"ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES\"] # je sais que c'est public mais changer ça est trop chiant à faire pzrce que faudra changer le nom des thèmes (ou y'a la faute aussi)\n",
    "    \n",
    "        writer = pd.ExcelWriter(fichier+'.xlsx', engine='xlsxwriter')\n",
    "    \n",
    "        resultat_1.to_excel(writer, sheet_name=\"DEMOCRATIE\")\n",
    "        resultat_2.to_excel(writer, sheet_name=\"FISCALITE\")\n",
    "        resultat_3.to_excel(writer, sheet_name=\"TRANSITION_ECOLOGIQUE\")\n",
    "        resultat_4.to_excel(writer, sheet_name=\"ORGANISATION_DE_LETAT\") \n",
    "    \n",
    "        writer.save()\n",
    "    return(resultat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va prendre un exemple pour essayer notre code: toutes les contributions du GDN qui contiennent les mots pandémie, épidémie sras, coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona=compte_group(contributions,r\"\\bpand[e-é]mi[a-z]\\b|\\b[e-é]pid[e-é]mie\\b|\\bsras\\b|\\bcoronavirus\\b\")\n",
    "\n",
    "aleo=corona[\"Réponse\"].apply(nltk.sent_tokenize)\n",
    "\n",
    "#DF series.apply est plus rapide que df.apply\n",
    "\n",
    "#On ne  prend que les contributions de plus d'une phrase\n",
    "essai=pd.DataFrame(aleo.loc[aleo.apply(len)>1].apply(pd.Series).stack()).reset_index()#la fonction series fait que le séparateur devient la virgule entre chaque éléments de la liste, le stack fait qu'on les empile, on a un multi index où chaque index de level 0= numéro de la contribution, level 1= numéro de la phrase dans la contribution\n",
    "\n",
    "#On ajoute le nombre de phrases de chaque contributions\n",
    "essai_gpby=essai.groupby(by=\"level_0\",as_index=False)[0].count()\n",
    "\n",
    "essai=essai.merge(essai_gpby,on=\"level_0\")\n",
    "\n",
    "\n",
    "essai.columns=[\"level_0\",\"nb_sent\",\"sent\",\"nb_sent_total\"]\n",
    "contain=essai.loc[essai[\"sent\"].str.contains(r\"\\bpand[e-é]mi[a-z]\\b|\\b[e-é]pid[e-é]mie\\b|\\bsras\\b|\\bcoronavirus\\b\",regex=True,case=False)]\n",
    "\n",
    "# On met dans un dictionnaire l'index de chaque phrase contenant notre pattern\n",
    "dico_lvl_contain=dict(zip(contain.index,zip(contain[\"nb_sent\"],contain[\"nb_sent_total\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque phrase qui contient les mots qu'on veut, on prend celle d'avant et celle d'après. \n",
    "Si jamais la phrase qui contient les patterns est la première, python va renvoyer une erreur lorsqu'on va lui demander \n",
    "de prendre la phrase d'avant vu qu'il n'y a rien avant, viceversa si c'est après. \n",
    "Pour résoudre cela, on va dire à python d'exclure la première lorsqu'on lui demandera de prendre les phrases x-1 et la dernière phrase quand on lui demande de prendre les phrases x+1 (encore une fois si y'a rien après x on va avoir une erreur index out of bounds). \n",
    "On a selectionné dans une autre opérationles phrases qui contiennent le pattern, cette opération a pour but de prendre x-1 et x+1 pas x. \n",
    "Si la première phrase contient le pattern, python ne va prendre que celle qui la suit, et si dernière phrase contient le pattern, python ne va prendre que celle qui la précède, \n",
    "Si la première et la deuxième phrase contiennent le patern, python va prendre celle qui la suit (x+1), et la ligne suivante python va de nouveau prendre la première (x-1, qui contient le pattern),  et la troisème (x+1), la deuxième étant x sur cette ligne (x+1 la ligne précédente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_avant=[[k-1,k,v] for (k,v) in dico_lvl_contain.items() if v[0]>0] #prend la phrase d'avant si la phrase  (celle qui matche le pattern) n'est pas la première de la contribution, donc d'index supérieur à 0\n",
    "phrase_avant=essai.loc[[x[0] for x in idx_avant],]\n",
    "phrase_avant[\"id_x\"]=essai.loc[[x[1] for x in idx_avant],].index # On ajoute l'index des phrases qui contiennent la phrase du patern(celle d'après du coup si répond aux conditions du for)\n",
    "phrase_avant.columns=[\"level_0\",\"nb_sent_av\", \"sent_av\",\"nb_sent_total\",\"id_x\"]\n",
    "\n",
    "idx_apres=[[k+1,k,v] for (k,v) in dico_lvl_contain.items() if v[0]<(v[1]-1)] # # prend la phrase d'après si la phrase n'est pas la dernière de la contribution, donc d'index inférieure au nombre de phrases dans la contribution. on ajoute moins 1 parce que v[1] c'est la taille par ex 20 et v[0] c'est l'index (0:19) donc faut mettre v[1]-1 pour que ça corresponde\n",
    "\n",
    "phrase_apres=essai.loc[[x[0] for x in idx_apres],]\n",
    "phrase_apres[\"id_x\"]=essai.loc[[x[1] for x in idx_apres],].index #On ajoute l'index des phrases du pattern (celle d'avant du coup si matche les conditions)\n",
    "phrase_apres.columns=[\"level_0\",\"nb_sent_ap\", \"sent_ap\",\"nb_sent_total\",\"id_x\"]\n",
    "contain=contain.rename_axis('id_x').reset_index()\n",
    "essai_final=contain.merge(phrase_avant, on=\"id_x\", how=\"outer\").merge(phrase_apres,on=\"id_x\",how=\"outer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
